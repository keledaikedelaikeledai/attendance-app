name: Deploy to demo via Tailscale

on:
  push:
    branches:
      - demo

permissions:
  contents: read

jobs:
  deploy:
    name: Build & deploy to Tailnet host
    runs-on: ubuntu-latest
    environment: demo
    env:
      DEPLOY_ENV: demo
      TARGET_SSH_PORT: 8022
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          version: latest

      - name: Install dependencies
        run: bun install

      - name: Build (Nuxt + Nitro)
        run: bun run build -- --preset bun

      - name: Prepare deploy archive
        run: |
          # Create a stable snapshot of tracked files (git archive) into a temp dir,
          # then add build/runtime artifacts (.output, public) if present and compress.
          rm -rf deploy_tmp deploy.tar.gz
          mkdir -p deploy_tmp

          # Export tracked files at HEAD to the temp dir
          git archive HEAD | tar -x -C deploy_tmp

          # Copy build artifacts produced by the previous steps (if any)
          if [ -d .output ]; then
            cp -a .output deploy_tmp/.output
          fi
          if [ -d public ]; then
            cp -a public deploy_tmp/public
          fi

          # Create compressed archive from the stable temp dir
          tar -czf deploy.tar.gz -C deploy_tmp .
          rm -rf deploy_tmp

      - name: Install & start Tailscale
        run: |
          set -euo pipefail
          # Install tailscale binaries
          curl -fsSL https://tailscale.com/install.sh | sh

          # Start tailscaled only if not already running (avoid socket-in-use)
          if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
            echo "tailscaled already running; skipping start"
          else
            echo "Starting tailscaled..."
            sudo tailscaled --tun=userspace-networking &>/tmp/tailscaled.log &
            # wait for the socket or process to show up
            for i in 1 2 3 4 5; do
              if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
                break
              fi
              sleep 1
            done
          fi

          # Try to bring up tailscale using the provided auth key with retries.
          MAX_ATTEMPTS=3
          ATTEMPT=1
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS: tailscale up --authkey (non-interactive)"
            if sudo tailscale up --authkey "${TAILSCALE_AUTHKEY}" --hostname "github-runner-${{ github.run_id }}" 2>/tmp/tailscale_up.err; then
              echo "tailscale up succeeded"
              break
            else
              echo "tailscale up failed on attempt $ATTEMPT; dumping stderr (short):"
              tail -n +1 /tmp/tailscale_up.err || true
              if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
                echo "ERROR: The provided TAILSCALE_AUTHKEY appears invalid or expired. Create a reusable auth key in the Tailscale admin console and add it to GitHub Secrets as TAILSCALE_AUTHKEY." >&2
                # No point retrying invalid key
                break
              fi
              ATTEMPT=$((ATTEMPT+1))
              sleep $((ATTEMPT*2))
            fi
          done

          echo "--- tailscaled log (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscaled.log || true'
          echo "--- tailscale up stderr (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscale_up.err || true'

          echo "--- tailscale status ---"
          sudo tailscale status || true
          echo "--- tailscale ip -4 ---"
          sudo tailscale ip -4 || true

          # If we ended with an 'invalid key' message, exit non-zero
          if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
            echo "Final error: TAILSCALE_AUTHKEY invalid or rejected. Aborting." >&2
            exit 1
          fi

        env:
          TAILSCALE_AUTHKEY: ${{ secrets.TAILSCALE_AUTHKEY }}

      - name: Configure SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          # disable strict host key checking for CI (optional; consider adding known_hosts)
          printf "Host *\n  StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null\n" > ~/.ssh/config

      - name: Ensure remote deploy directory exists
        run: |
          # If TARGET_DEPLOY_PATH is provided, create that path; otherwise create a path under the remote user's $HOME
          REMOTE_DIR_RUNNER="${TARGET_DEPLOY_PATH:-}"
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if [ -n "$REMOTE_DIR_RUNNER" ]; then
            echo "Ensuring remote dir: $REMOTE_DIR_RUNNER (ssh port: $SSH_PORT)"
            MKDIR_CMD="mkdir -p '$REMOTE_DIR_RUNNER'"
          else
            # Use a home-relative default on the remote side to avoid trying to write to / on proot/termux
            echo "No TARGET_DEPLOY_PATH provided; will create directory under remote user's HOME: \$HOME/app/attendance/${DEPLOY_ENV} (ssh port: $SSH_PORT)"
            MKDIR_CMD="mkdir -p \"\$HOME/app/attendance/${DEPLOY_ENV}\""
          fi
          # Prevent the ssh call from hanging indefinitely; timeout after 20s
          if ! timeout 20s ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 ${TARGET_SSH_USER}@${TARGET_HOST} "$MKDIR_CMD" 2>/tmp/ssh_mkdir.err; then
            cat /tmp/ssh_mkdir.err || true
            if grep -q "Tailscale SSH requires an additional check" /tmp/ssh_mkdir.err 2>/dev/null || grep -q "Authentication check" /tmp/ssh_mkdir.err 2>/dev/null; then
              echo "ERROR: The target host is enforcing Tailscale SSH SSO and requires interactive authentication. CI cannot proceed non-interactively.\nOptions:\n - Use a host with standard OpenSSH and a deploy public key in ~/.ssh/authorized_keys\n - Configure Tailscale to allow machine authentication for CI or use a different deployment method." >&2
            else
              echo "ERROR: ssh to ${TARGET_HOST} failed or timed out. See /tmp/ssh_mkdir.err for details." >&2
            fi
            exit 1
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify Tailscale connectivity to target host
        run: |
          # Try a quick non-interactive SSH check to ensure the Tailscale route is available
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if ssh -p "$SSH_PORT" -o BatchMode=yes -o ConnectTimeout=10 -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} 'echo ok' 2>/dev/null; then
            echo "Target reachable over Tailscale"
          else
            echo "ERROR: Target ${TARGET_HOST} is not reachable over Tailscale from the runner. Ensure the TAILSCALE_AUTHKEY is valid and the host is online." >&2
            exit 1
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}

      - name: Copy archive to target host over Tailscale
        run: |
          # If TARGET_DEPLOY_PATH is set, use it; otherwise use a home-relative path (~ expands on remote shell)
          if [ -n "${TARGET_DEPLOY_PATH:-}" ]; then
            REMOTE_PATH="${TARGET_DEPLOY_PATH}/deploy.tar.gz"
          else
            REMOTE_PATH="~/app/attendance/${DEPLOY_ENV}/deploy.tar.gz"
          fi
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          echo "Copying archive to ${TARGET_SSH_USER}@${TARGET_HOST}:$REMOTE_PATH (scp port: $SSH_PORT)"
          scp -P "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 deploy.tar.gz ${TARGET_SSH_USER}@${TARGET_HOST}:"$REMOTE_PATH" 2>/tmp/scp.err || {
            cat /tmp/scp.err || true
            echo "ERROR: scp failed. Check that the deploy user has a matching public key and that Tailscale connectivity is available." >&2
            exit 1
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify uploaded archive exists on target
        run: |
          if [ -n "${TARGET_DEPLOY_PATH:-}" ]; then
            REMOTE_PATH="${TARGET_DEPLOY_PATH}/deploy.tar.gz"
          else
            REMOTE_PATH="~/app/attendance/${DEPLOY_ENV}/deploy.tar.gz"
          fi
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          echo "Verifying archive exists at $REMOTE_PATH (ssh port: $SSH_PORT)"
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 ${TARGET_SSH_USER}@${TARGET_HOST} "[ -f '$REMOTE_PATH' ] && echo OK || (echo 'ERROR: deploy.tar.gz not found on target' >&2; exit 1)" 2>/tmp/ssh_verify.err || {
            cat /tmp/ssh_verify.err || true
            if grep -q "Tailscale SSH requires an additional check" /tmp/ssh_verify.err 2>/dev/null; then
              echo "ERROR: Tailscale SSH is requesting interactive SSO authentication. CI cannot continue. See docs: use standard OpenSSH+authorized_keys or configure machine auth for CI." >&2
            fi
            exit 1
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Extract, build and start with PM2 on target host
        run: |
          # Compute the remote directory on the runner so we inject a literal path into the remote script
          # If TARGET_DEPLOY_PATH is set, pass it; otherwise pass an empty arg and let the remote side default to $HOME
          if [ -n "${TARGET_DEPLOY_PATH:-}" ]; then
            REMOTE_ARG="${TARGET_DEPLOY_PATH}"
            echo "Using remote directory (explicit): $REMOTE_ARG"
          else
            REMOTE_ARG=""
            echo "No TARGET_DEPLOY_PATH provided; remote will default to \$HOME/app/attendance/${DEPLOY_ENV}"
          fi
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "About to run: ssh -p \"$SSH_PORT\" -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} 'DEPLOY_ENV=${DEPLOY_ENV} bash -s' -- '$REMOTE_ARG'"

          # Remote script: extract, install/build with Bun, and run via PM2 using Bun as the interpreter
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} "DEPLOY_ENV=${DEPLOY_ENV} bash -s" -- "$REMOTE_ARG" <<'SSH'
            set -euo pipefail
            # Accept an optional argument; if empty, default to $HOME/app/attendance/${DEPLOY_ENV}
            ARG="$1"
            if [ -n "$ARG" ]; then
              TARGET_DIR="$ARG"
            else
              TARGET_DIR="$HOME/app/attendance/${DEPLOY_ENV}"
            fi
            printf 'REMOTE: Resolved TARGET_DIR=<%s>\n' "$TARGET_DIR"
            printf 'REMOTE: TARGET_DIR length: %d\n' "$(printf '%s' "$TARGET_DIR" | wc -c)"
            mkdir -p "$TARGET_DIR"
            cd "$TARGET_DIR"

            # extract uploaded archive into target dir (scp copied deploy.tar.gz)
            if [ -f "$TARGET_DIR/deploy.tar.gz" ]; then
              tar -xzf "$TARGET_DIR/deploy.tar.gz" -C "$TARGET_DIR"
              rm -f "$TARGET_DIR/deploy.tar.gz"
            else
              echo "WARNING: deploy.tar.gz not found in $TARGET_DIR â€” continuing if files already present"
            fi

            # Ensure Bun is available (install locally to $HOME/.bun if needed)
            BUN_BIN="$HOME/.bun/bin/bun"
            if command -v bun >/dev/null 2>&1; then
              BUN_BIN="$(command -v bun)"
            elif [ -x "$BUN_BIN" ]; then
              true
            else
              echo "Bun not found; installing to $HOME/.bun (non-interactive)"
              curl -fsSL https://bun.sh/install | bash -s -- --bun-install-dir "$HOME/.bun" || true
              export PATH="$HOME/.bun/bin:$PATH"
              BUN_BIN="$HOME/.bun/bin/bun"
            fi

            # Install dependencies (prefer production install; fall back to full if needed)
            if [ -f package.json ]; then
              echo "Running: $BUN_BIN install --production"
              if ! $BUN_BIN install --production >/tmp/bun_install.out 2>&1; then
                echo "Production install failed; trying full install (logs in /tmp/bun_install.out)"
                tail -n 200 /tmp/bun_install.out || true
                $BUN_BIN install || true
              fi
            fi

            # Build the project (Nitro) if build script exists
            if grep -q '"build"' package.json 2>/dev/null || grep -q 'build' package.json 2>/dev/null; then
              echo "Running build on remote host"
              $BUN_BIN run build -- --preset bun || (echo 'Build failed' >&2; exit 1)
            fi

            # Ensure pm2 is available (install globally via npm if absent)
            if ! command -v pm2 >/dev/null 2>&1; then
              if command -v npm >/dev/null 2>&1; then
                echo "Installing pm2 globally via npm"
                npm install -g pm2 || true
              else
                echo "pm2 not found and npm not available; cannot start process manager" >&2
              fi
            fi

            PM2_NAME="attendance-app"
            # Stop and remove existing process if present
            if command -v pm2 >/dev/null 2>&1 && pm2 describe "$PM2_NAME" >/dev/null 2>&1; then
              pm2 stop "$PM2_NAME" || true
              pm2 delete "$PM2_NAME" || true
            fi

            # Start the app via pm2 using Bun as the interpreter
            BUN_PATH="$BUN_BIN"
            if [ ! -x "$BUN_PATH" ]; then
              BUN_PATH="$(command -v bun || echo "$BUN_PATH")"
            fi
            if command -v pm2 >/dev/null 2>&1; then
              echo "Starting app with pm2 (interpreter: $BUN_PATH)"
              pm2 start --name "$PM2_NAME" --interpreter "$BUN_PATH" ./.output/server/index.mjs --update-env || {
                echo "pm2 start failed" >&2
                exit 1
              }
              pm2 save || true
            else
              echo "pm2 not available; falling back to running directly with Bun in background"
              nohup "$BUN_PATH" ./.output/server/index.mjs &>/dev/null &
            fi
          SSH
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Notify
        run: echo "Deploy to ${TARGET_HOST} completed"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

      - name: Cleanup Tailscale and runner artifacts
        if: always()
        run: |
          echo "Cleaning up tailscale and temporary files"
          set -euo pipefail
          # Attempt to bring down tailscale and logout (idempotent)
          if command -v tailscale >/dev/null 2>&1; then
            sudo tailscale down || true
            sudo tailscale logout || true
          fi
          # Kill tailscaled if it's still running
          if pgrep -x tailscaled >/dev/null 2>&1; then
            sudo pkill -9 tailscaled || true
          fi
          # Remove tailscaled socket and temp logs we created
          sudo rm -f /var/run/tailscale/tailscaled.sock || true
          sudo rm -f /tmp/tailscaled.log /tmp/tailscale_up.err || true
          # Remove the private key from runner workspace
          rm -f ~/.ssh/id_rsa || true
          # Remove ssh config created earlier
          rm -f ~/.ssh/config || true
          echo "Cleanup finished"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

    # required secrets
    # secrets to set in repository settings:
    # - TAILSCALE_AUTHKEY
    # - SSH_PRIVATE_KEY (PEM formatted private key for TARGET_SSH_USER)
    # - TARGET_HOST (target device Tailscale IP or hostname reachable over tailnet)
    # - TARGET_SSH_USER (username to SSH as)
    # - TARGET_DEPLOY_PATH (optional remote path, defaults to /home/<user>/deploy)
