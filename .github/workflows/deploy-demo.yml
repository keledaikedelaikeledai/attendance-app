name: Deploy to demo via Tailscale

on:
  push:
    branches:
      - demo

permissions:
  contents: read

jobs:
  deploy:
    name: Build & deploy to Tailnet host
    runs-on: ubuntu-latest
    environment: demo
    env:
      DEPLOY_ENV: demo
      TARGET_SSH_PORT: 8022
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          version: latest

      - name: Install dependencies
        run: bun install

      - name: Build (Nuxt + Nitro)
        run: bun run build -- --preset bun

      - name: Prepare deploy archive
        run: |
          # Create a stable snapshot of tracked files (git archive) into a temp dir,
          # then add build/runtime artifacts (.output, public) if present and compress.
          rm -rf deploy_tmp deploy.tar.gz
          mkdir -p deploy_tmp

          # Export tracked files at HEAD to the temp dir
          git archive HEAD | tar -x -C deploy_tmp

          # Copy build artifacts produced by the previous steps (if any)
          if [ -d .output ]; then
            cp -a .output deploy_tmp/.output
          fi
          if [ -d public ]; then
            cp -a public deploy_tmp/public
          fi

          # Create compressed archive from the stable temp dir
          tar -czf deploy.tar.gz -C deploy_tmp .
          rm -rf deploy_tmp

      - name: Install & start Tailscale
        run: |
          set -euo pipefail
          # Install tailscale binaries
          curl -fsSL https://tailscale.com/install.sh | sh

          # Start tailscaled only if not already running (avoid socket-in-use)
          if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
            echo "tailscaled already running; skipping start"
          else
            echo "Starting tailscaled..."
            sudo tailscaled --tun=userspace-networking &>/tmp/tailscaled.log &
            # wait for the socket or process to show up
            for i in 1 2 3 4 5; do
              if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
                break
              fi
              sleep 1
            done
          fi

          # Try to bring up tailscale using the provided auth key with retries.
          MAX_ATTEMPTS=3
          ATTEMPT=1
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS: tailscale up --authkey (non-interactive)"
            if sudo tailscale up --authkey "${TAILSCALE_AUTHKEY}" --hostname "github-runner-${{ github.run_id }}" 2>/tmp/tailscale_up.err; then
              echo "tailscale up succeeded"
              break
            else
              echo "tailscale up failed on attempt $ATTEMPT; dumping stderr (short):"
              tail -n +1 /tmp/tailscale_up.err || true
              if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
                echo "ERROR: The provided TAILSCALE_AUTHKEY appears invalid or expired. Create a reusable auth key in the Tailscale admin console and add it to GitHub Secrets as TAILSCALE_AUTHKEY." >&2
                # No point retrying invalid key
                break
              fi
              ATTEMPT=$((ATTEMPT+1))
              sleep $((ATTEMPT*2))
            fi
          done

          echo "--- tailscaled log (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscaled.log || true'
          echo "--- tailscale up stderr (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscale_up.err || true'

          echo "--- tailscale status ---"
          sudo tailscale status || true
          echo "--- tailscale ip -4 ---"
          sudo tailscale ip -4 || true

          # If we ended with an 'invalid key' message, exit non-zero
          if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
            echo "Final error: TAILSCALE_AUTHKEY invalid or rejected. Aborting." >&2
            exit 1
          fi

        env:
          TAILSCALE_AUTHKEY: ${{ secrets.TAILSCALE_AUTHKEY }}

      - name: Configure SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          # disable strict host key checking for CI (optional; consider adding known_hosts)
          printf "Host *\n  StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null\n" > ~/.ssh/config

      - name: Ensure remote deploy directory exists
        run: |
          # If TARGET_DEPLOY_PATH is provided, create that path; otherwise create a path under the remote user's $HOME
          REMOTE_DIR_RUNNER="${TARGET_DEPLOY_PATH:-}"
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if [ -n "$REMOTE_DIR_RUNNER" ]; then
            echo "Ensuring remote dir: $REMOTE_DIR_RUNNER (ssh port: $SSH_PORT)"
            # If an absolute path was provided, don't try to create it directly on Termux's / (may be read-only).
            # Prefer creating the absolute path inside the Ubuntu proot via proot-distro; if that fails, fall back to creating
            # a home-prefixed path instead ($HOME/<path-without-leading-slash>) so Termux user can write it.
            if case "$REMOTE_DIR_RUNNER" in /*) true ;; *) false ;; esac; then
              # Note: we escape $HOME as \$HOME so it is evaluated on the remote side, not by the runner
              MKDIR_CMD="proot-distro login ubuntu -- mkdir -p '$REMOTE_DIR_RUNNER' || mkdir -p \"\$HOME/${REMOTE_DIR_RUNNER#/}\""
            else
              MKDIR_CMD="mkdir -p '$REMOTE_DIR_RUNNER'"
            fi
          else
            # Use a home-relative default on the remote side to avoid trying to write to / on proot/termux
            echo "No TARGET_DEPLOY_PATH provided; will create directory under remote user's HOME: \$HOME/app/attendance/${DEPLOY_ENV} (ssh port: $SSH_PORT)"
            MKDIR_CMD="mkdir -p \"\$HOME/app/attendance/${DEPLOY_ENV}\""
          fi
          # Prevent the ssh call from hanging indefinitely; increase timeout to 60s to allow slow wakeups
          # Capture verbose SSH stderr into /tmp/ssh_mkdir.err for debugging
          if ! timeout 60s ssh -vvv -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} "$MKDIR_CMD" 2>/tmp/ssh_mkdir.err; then
            echo "--- SSH mkdir stderr (/tmp/ssh_mkdir.err) ---"
            sed -n '1,200p' /tmp/ssh_mkdir.err || true
            echo "--- end stderr ---"

            # Look for known interactive SSO prompts in the ssh stderr
            if grep -qi "Tailscale SSH requires an additional check" /tmp/ssh_mkdir.err 2>/dev/null || grep -qi "Authentication check" /tmp/ssh_mkdir.err 2>/dev/null; then
              echo "ERROR: The target host is enforcing Tailscale SSH SSO and requires interactive authentication. CI cannot proceed non-interactively.\nOptions:\n - Use a host with standard OpenSSH and a deploy public key in ~/.ssh/authorized_keys\n - Configure Tailscale to allow machine authentication for CI or use a different deployment method." >&2
            else
              echo "ERROR: ssh to ${TARGET_HOST} failed or timed out. See /tmp/ssh_mkdir.err above for verbose SSH output." >&2
            fi

            echo "Running an additional non-destructive SSH diagnostic (whoami, HOME, id, uname) to help debug the target environment";
            # Capture diagnostic output (may also fail) to /tmp/ssh_diag.out
            if ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} 'whoami; echo HOME="$HOME"; id; uname -a' 2>/tmp/ssh_diag.err | sed -n '1,200p'; then
              echo "--- SSH diagnostic succeeded ---"
              sed -n '1,200p' /tmp/ssh_diag.err || true
            else
              echo "--- SSH diagnostic failed; stderr (first 200 lines): ---"
              sed -n '1,200p' /tmp/ssh_diag.err || true
            fi

            exit 1
          fi
          # Also create a matching directory inside the Ubuntu proot (if proot-distro exists)
          echo "Attempting to create distro-side target inside proot (if available)"
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 ${TARGET_SSH_USER}@${TARGET_HOST} "proot-distro login ubuntu -- mkdir -p '/root/app/attendance/${DEPLOY_ENV}'" || true
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify Tailscale connectivity to target host
        run: |
          # Try a quick non-interactive SSH check to ensure the Tailscale route is available
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if ssh -p "$SSH_PORT" -o BatchMode=yes -o ConnectTimeout=10 -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} 'echo ok' 2>/dev/null; then
            echo "Target reachable over Tailscale"
          else
            echo "ERROR: Target ${TARGET_HOST} is not reachable over Tailscale from the runner. Ensure the TAILSCALE_AUTHKEY is valid and the host is online." >&2
            exit 1
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}

      - name: Copy archive to target host over Tailscale
        run: |
          # If TARGET_DEPLOY_PATH is absolute (starts with /) we will copy into $HOME/<path-without-leading-slash>
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if [ -n "${TARGET_DEPLOY_PATH:-}" ]; then
            if case "${TARGET_DEPLOY_PATH}" in /*) true ;; *) false ;; esac; then
              echo "TARGET_DEPLOY_PATH is absolute; copying into remote user's HOME under that path"
              # Use remote $HOME expansion by quoting $HOME in the scp target
              scp -P "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 deploy.tar.gz ${TARGET_SSH_USER}@${TARGET_HOST}:'$HOME/'"${TARGET_DEPLOY_PATH#/}"/deploy.tar.gz" 2>/tmp/scp.err || {
                cat /tmp/scp.err || true
                echo "ERROR: scp failed. Check that the deploy user has a matching public key and that Tailscale connectivity is available." >&2
                exit 1
              }
            else
              echo "Copying archive to ${TARGET_DEPLOY_USER}@${TARGET_HOST}:${TARGET_DEPLOY_PATH}/deploy.tar.gz (scp port: $SSH_PORT)"
              scp -P "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 deploy.tar.gz ${TARGET_SSH_USER}@${TARGET_HOST}:"${TARGET_DEPLOY_PATH}/deploy.tar.gz" 2>/tmp/scp.err || {
                cat /tmp/scp.err || true
                echo "ERROR: scp failed. Check that the deploy user has a matching public key and that Tailscale connectivity is available." >&2
                exit 1
              }
            fi
          else
            echo "No TARGET_DEPLOY_PATH provided; copying to ~/app/attendance/${DEPLOY_ENV}/deploy.tar.gz"
            scp -P "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 deploy.tar.gz ${TARGET_SSH_USER}@${TARGET_HOST}:'$HOME/app/attendance/${DEPLOY_ENV}/deploy.tar.gz' 2>/tmp/scp.err || {
              cat /tmp/scp.err || true
              echo "ERROR: scp failed. Check that the deploy user has a matching public key and that Tailscale connectivity is available." >&2
              exit 1
            }
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify uploaded archive exists on target
        run: |
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if [ -n "${TARGET_DEPLOY_PATH:-}" ]; then
            if case "${TARGET_DEPLOY_PATH}" in /*) true ;; *) false ;; esac; then
              REMOTE_TEST_CMD='[ -f "$HOME/'"${TARGET_DEPLOY_PATH#/}"'/deploy.tar.gz" ] && echo OK || (echo "ERROR: deploy.tar.gz not found on target" >&2; exit 1)'
            else
              REMOTE_TEST_CMD='[ -f "'"${TARGET_DEPLOY_PATH}"'/deploy.tar.gz" ] && echo OK || (echo "ERROR: deploy.tar.gz not found on target" >&2; exit 1)'
            fi
          else
            REMOTE_TEST_CMD='[ -f "$HOME/app/attendance/${DEPLOY_ENV}/deploy.tar.gz" ] && echo OK || (echo "ERROR: deploy.tar.gz not found on target" >&2; exit 1)'
          fi
          echo "Verifying archive exists on remote (ssh port: $SSH_PORT)"
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 ${TARGET_SSH_USER}@${TARGET_HOST} "$REMOTE_TEST_CMD" 2>/tmp/ssh_verify.err || {
            cat /tmp/ssh_verify.err || true
            if grep -q "Tailscale SSH requires an additional check" /tmp/ssh_verify.err 2>/dev/null; then
              echo "ERROR: Tailscale SSH is requesting interactive SSO authentication. CI cannot continue. See docs: use standard OpenSSH+authorized_keys or configure machine auth for CI." >&2
            fi
            exit 1
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Extract, build and start with PM2 on target host
        run: |
          # Compute the remote directory on the runner so we inject a literal path into the remote script
          # Pass TARGET_DEPLOY_PATH into the remote environment; the remote script will resolve to $HOME when appropriate
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "About to run remote extract/build/start (ssh port: $SSH_PORT)"

          # Remote script: move archive from Termux home into distro and run everything inside proot
          # We'll run proot-distro login ubuntu -- bash -lc '...remote commands ...'
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} "proot-distro login ubuntu -- bash -lc 'set -euo pipefail; \
            # Resolve target dir inside the distro; prefer TARGET_DEPLOY_PATH if non-empty; default to /root/app/attendance/${DEPLOY_ENV} inside distro; \
            if [ -n "'"${TARGET_DEPLOY_PATH:-}''" ]; then \
              case "'"${TARGET_DEPLOY_PATH}''" in /*) TDIR=\"/root/'"${TARGET_DEPLOY_PATH#/}'\" ;; *) TDIR=\"'"${TARGET_DEPLOY_PATH}'\" ;; esac; \
            else \
              TDIR=\"/root/app/attendance/${DEPLOY_ENV}\"; \
            fi; \
            echo PROOT: resolved distro TARGET_DIR=\"$TDIR\"; \
            mkdir -p "$HOME/app/attendance/${DEPLOY_ENV}" || true; \
            # If archive exists in Termux home, copy into distro temporary path and extract; otherwise failing is acceptable\"'" || true
          # Now run the main proot script which will perform the copy and run bun inside the distro
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} "proot-distro login ubuntu -- bash -lc 'set -euo pipefail; \n            # ensure distro target dir exists\n            if [ -n \"'"${TARGET_DEPLOY_PATH:-}\"'\" ]; then\n              case \"'"${TARGET_DEPLOY_PATH}\"' in /*) TDIR=\"/root/'"${TARGET_DEPLOY_PATH#/}'\" ;; *) TDIR=\"'"${TARGET_DEPLOY_PATH}'\" ;; esac\n            else\n              TDIR=\"/root/app/attendance/${DEPLOY_ENV}\"\n            fi\n            echo PROOT: using TDIR=\"$TDIR\"\n            mkdir -p \"$TDIR\"\n            # copy archive from Termux home into distro (Termux HOME relative path)\n            if [ -f \"$HOME/app/attendance/${DEPLOY_ENV}/deploy.tar.gz\" ]; then\n              cp \"$HOME/app/attendance/${DEPLOY_ENV}/deploy.tar.gz\" \"/tmp/deploy_${DEPLOY_ENV}.tar.gz\" || true\n              tar -xzf \"/tmp/deploy_${DEPLOY_ENV}.tar.gz\" -C \"$TDIR\" || true\n              rm -f \"/tmp/deploy_${DEPLOY_ENV}.tar.gz\" || true\n            else\n              echo PROOT: no archive found in Termux home; continuing if files already present in distro\n            fi\n            cd \"$TDIR\"\n            # ensure bun exists inside distro\n            if command -v bun >/dev/null 2>&1; then BUN=\"$(command -v bun)\"; else echo PROOT: bun missing in distro; fi\n            # Install deps inside distro if package.json exists\n            if [ -f package.json ]; then\n              echo PROOT: running bun install inside distro\n              bun install || true\n            fi\n            # build inside distro if build script exists\n            if [ -f package.json ] && grep -q '"build"' package.json 2>/dev/null; then\n              bun run build -- --preset bun || true\n            fi\n            # start via pm2 inside distro\n            if command -v pm2 >/dev/null 2>&1; then\n              pm2 stop attendance-app || true; pm2 delete attendance-app || true; pm2 start --name attendance-app --interpreter \"$BUN\" ./.output/server/index.mjs --update-env || true; pm2 save || true\n            else\n              echo PROOT: pm2 missing; attempting to run bun directly in background\n              nohup \"$BUN\" ./.output/server/index.mjs &>/dev/null &\n            fi'" || true
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Notify
        run: echo "Deploy to ${TARGET_HOST} completed"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

      - name: Cleanup Tailscale and runner artifacts
        if: always()
        run: |
          echo "Cleaning up tailscale and temporary files"
          set -euo pipefail
          # Attempt to bring down tailscale and logout (idempotent)
          if command -v tailscale >/dev/null 2>&1; then
            sudo tailscale down || true
            sudo tailscale logout || true
          fi
          # Kill tailscaled if it's still running
          if pgrep -x tailscaled >/dev/null 2>&1; then
            sudo pkill -9 tailscaled || true
          fi
          # Remove tailscaled socket and temp logs we created
          sudo rm -f /var/run/tailscale/tailscaled.sock || true
          sudo rm -f /tmp/tailscaled.log /tmp/tailscale_up.err || true
          # Remove the private key from runner workspace
          rm -f ~/.ssh/id_rsa || true
          # Remove ssh config created earlier
          rm -f ~/.ssh/config || true
          echo "Cleanup finished"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

    # required secrets
    # secrets to set in repository settings:
    # - TAILSCALE_AUTHKEY
    # - SSH_PRIVATE_KEY (PEM formatted private key for TARGET_SSH_USER)
    # - TARGET_HOST (target device Tailscale IP or hostname reachable over tailnet)
    # - TARGET_SSH_USER (username to SSH as)
    # - TARGET_DEPLOY_PATH (optional remote path, defaults to /home/<user>/deploy)
