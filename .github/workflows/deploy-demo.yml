name: Deploy to demo via Tailscale

on:
  push:
    branches:
      - demo

permissions:
  contents: read

jobs:
  deploy:
    name: Build & deploy to Tailnet host
    runs-on: ubuntu-latest
    environment: demo
    env:
      DEPLOY_ENV: demo
      TARGET_SSH_PORT: 8022
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          version: latest

      - name: Install dependencies
        run: bun install

      - name: Build (Nuxt + Nitro)
        run: bun run build -- --preset bun

      - name: Prepare deploy archive
        run: |
          # Create a stable snapshot of tracked files (git archive) into a temp dir,
          # then add build/runtime artifacts (.output, public) if present and compress.
          rm -rf deploy_tmp deploy.tar.gz
          mkdir -p deploy_tmp

          # Export tracked files at HEAD to the temp dir
          git archive HEAD | tar -x -C deploy_tmp

          # Copy build artifacts produced by the previous steps (if any)
          if [ -d .output ]; then
            cp -a .output deploy_tmp/.output
          fi
          if [ -d public ]; then
            cp -a public deploy_tmp/public
          fi

          # Create compressed archive from the stable temp dir
          tar -czf deploy.tar.gz -C deploy_tmp .
          rm -rf deploy_tmp

      - name: Install & start Tailscale
        run: |
          set -euo pipefail
          # Install tailscale binaries
          curl -fsSL https://tailscale.com/install.sh | sh

          # Start tailscaled only if not already running (avoid socket-in-use)
          if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
            echo "tailscaled already running; skipping start"
          else
            echo "Starting tailscaled..."
            sudo tailscaled --tun=userspace-networking &>/tmp/tailscaled.log &
            # wait for the socket or process to show up
            for i in 1 2 3 4 5; do
              if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
                break
              fi
              sleep 1
            done
          fi

          # Try to bring up tailscale using the provided auth key with retries.
          MAX_ATTEMPTS=3
          ATTEMPT=1
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS: tailscale up --authkey (non-interactive)"
            if sudo tailscale up --authkey "${TAILSCALE_AUTHKEY}" --hostname "github-runner-${{ github.run_id }}" 2>/tmp/tailscale_up.err; then
              echo "tailscale up succeeded"
              break
            else
              echo "tailscale up failed on attempt $ATTEMPT; dumping stderr (short):"
              tail -n +1 /tmp/tailscale_up.err || true
              if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
                echo "ERROR: The provided TAILSCALE_AUTHKEY appears invalid or expired. Create a reusable auth key in the Tailscale admin console and add it to GitHub Secrets as TAILSCALE_AUTHKEY." >&2
                # No point retrying invalid key
                break
              fi
              ATTEMPT=$((ATTEMPT+1))
              sleep $((ATTEMPT*2))
            fi
          done

          echo "--- tailscaled log (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscaled.log || true'
          echo "--- tailscale up stderr (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscale_up.err || true'

          echo "--- tailscale status ---"
          sudo tailscale status || true
          echo "--- tailscale ip -4 ---"
          sudo tailscale ip -4 || true

          # If we ended with an 'invalid key' message, exit non-zero
          if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
            echo "Final error: TAILSCALE_AUTHKEY invalid or rejected. Aborting." >&2
            exit 1
          fi

        env:
          TAILSCALE_AUTHKEY: ${{ secrets.TAILSCALE_AUTHKEY }}

      - name: Configure SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          # disable strict host key checking for CI (optional; consider adding known_hosts)
          printf "Host *\n  StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null\n" > ~/.ssh/config

      - name: Ensure remote deploy directory exists
        run: |
          # Create a simple home-staging directory on the remote so scp has a parent to write to.
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Ensuring remote home-staging directory exists (ssh port: $SSH_PORT)"
          REMOTE_HOME_STAGE='"$HOME"/app/attendance/${DEPLOY_ENV}'

          # Use a straightforward ssh mkdir -p on the remote user's $HOME to avoid touching Termux root
          if ! timeout 60s ssh -vvv -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} "mkdir -p \"\$HOME/app/attendance/${DEPLOY_ENV}\"" 2>/tmp/ssh_mkdir.err; then
            echo "--- SSH mkdir stderr (/tmp/ssh_mkdir.err) ---"
            sed -n '1,200p' /tmp/ssh_mkdir.err || true
            echo "--- end stderr ---"
            echo "ERROR: Failed to create remote home-staging directory. See output above." >&2
            echo "Running SSH diagnostic (whoami, HOME, id, uname) to help debug the remote environment"
            ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} 'whoami; echo HOME="$HOME"; id; uname -a' 2>/tmp/ssh_diag.err || true
            sed -n '1,200p' /tmp/ssh_diag.err || true
            exit 1
          fi

          echo "Remote home-staging directory ensured: \$HOME/app/attendance/${DEPLOY_ENV}"

          # Optionally attempt to create the distro-side directory inside proot (best-effort)
          echo "Attempting to create distro-side target inside proot (if available)"
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} "proot-distro login ubuntu -- mkdir -p '/root/app/attendance/${DEPLOY_ENV}'" 2>/tmp/ssh_proot.err || {
            echo "proot mkdir attempt failed (non-fatal). proot stderr:"; sed -n '1,200p' /tmp/ssh_proot.err || true; true
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify Tailscale connectivity to target host
        run: |
          # Try a quick non-interactive SSH check to ensure the Tailscale route is available
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          if ssh -p "$SSH_PORT" -o BatchMode=yes -o ConnectTimeout=10 -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} 'echo ok' 2>/dev/null; then
            echo "Target reachable over Tailscale"
          else
            echo "ERROR: Target ${TARGET_HOST} is not reachable over Tailscale from the runner. Ensure the TAILSCALE_AUTHKEY is valid and the host is online." >&2
            exit 1
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}

      - name: Copy archive to target host over Tailscale
        run: |
          # Always copy the archive into a home-staging directory on the remote (Termux home).
          # The subsequent Extract step runs inside the distro and will copy/move the archive into the distro as needed.
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"
          # Query the remote user's HOME explicitly to avoid quoting problems
          echo "Querying remote HOME via SSH"
          REMOTE_HOME=$(ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} 'printf "%s" "$HOME"' 2>/tmp/ssh_home.err) || {
            echo "Failed to query remote HOME; ssh stderr:"; sed -n '1,200p' /tmp/ssh_home.err || true
            echo "Cannot proceed without knowing remote HOME" >&2
            exit 1
          }
          echo "Remote HOME: $REMOTE_HOME"

          REMOTE_STAGE_DIR="${REMOTE_HOME}/app/attendance/${DEPLOY_ENV}"
          echo "Ensuring remote staging dir exists: $REMOTE_STAGE_DIR"
          if ! ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} "mkdir -p \"${REMOTE_STAGE_DIR}\"" 2>/tmp/ssh_mkdir_for_scp.err; then
            echo "Failed to create remote staging dir; stderr:"; sed -n '1,200p' /tmp/ssh_mkdir_for_scp.err || true
            exit 1
          fi

          echo "Copying deploy.tar.gz to remote staging: ${REMOTE_STAGE_DIR}/deploy.tar.gz (ssh port: $SSH_PORT)"
          scp -P "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 deploy.tar.gz ${TARGET_SSH_USER}@${TARGET_HOST}:"${REMOTE_STAGE_DIR}/deploy.tar.gz" 2>/tmp/scp.err || {
            echo "--- scp stderr ---"
            sed -n '1,200p' /tmp/scp.err || true
            echo "--- end scp stderr ---"
            echo "ERROR: scp failed. Check that the deploy user has a matching public key, the host is reachable over Tailscale, and the remote path is writable." >&2
            exit 1
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify uploaded archive exists on target
        run: |
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "Raw TARGET_SSH_PORT: '${TARGET_SSH_PORT:-<unset>}'"

          # Query remote HOME so we can check a concrete path (matches the copy step behavior)
          REMOTE_HOME=$(ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} 'printf "%s" "$HOME"' 2>/tmp/ssh_home_verify.err) || {
            echo "Failed to determine remote HOME; ssh stderr:"; sed -n '1,200p' /tmp/ssh_home_verify.err || true
            exit 1
          }
          echo "Remote HOME for verify: $REMOTE_HOME"

          CHECK_PATH="${REMOTE_HOME}/app/attendance/${DEPLOY_ENV}/deploy.tar.gz"
          echo "Verifying archive exists at: $CHECK_PATH"

          if ! ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} "[ -f \"${CHECK_PATH}\" ]" 2>/tmp/ssh_verify.err; then
            echo "ERROR: deploy.tar.gz not found on target at ${CHECK_PATH}" >&2
            echo "Remote staging dir listing (for debugging):"
            ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 ${TARGET_SSH_USER}@${TARGET_HOST} "ls -la \"${REMOTE_HOME}/app/attendance/${DEPLOY_ENV}\" || true" 2>/tmp/ssh_verify_ls.err || true
            echo "--- ls stderr (if any) ---"
            sed -n '1,200p' /tmp/ssh_verify_ls.err || true
            echo "--- end ls stderr ---"
            cat /tmp/ssh_verify.err || true
            exit 1
          fi
          echo "OK: deploy.tar.gz found at ${CHECK_PATH}"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Extract, build and start with PM2 on target host
        run: |
          SSH_PORT="${TARGET_SSH_PORT:-8022}"
          echo "About to run remote extract/build/start (ssh port: $SSH_PORT)"

          # Run a remote script via SSH. We pass DEPLOY_ENV into the remote shell so it's available there and inside the proot heredoc.
          ssh -p "$SSH_PORT" -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} "DEPLOY_ENV='${DEPLOY_ENV}' bash -s" <<'SSH'
            set -euo pipefail
            echo "REMOTE: running extract/build/start (DEPLOY_ENV=$DEPLOY_ENV)"
            REMOTE_HOME="$HOME"
            STAGE="$REMOTE_HOME/app/attendance/${DEPLOY_ENV}"
            mkdir -p "$STAGE"

            if [ -f "$STAGE/deploy.tar.gz" ]; then
              echo "REMOTE: archive found in home-staging: $STAGE/deploy.tar.gz"
              # Run a script inside the Ubuntu proot distro to extract, build and start
              proot-distro login ubuntu -- bash -s <<'PROOT'
                set -euo pipefail
                echo "PROOT: running inside distro (DEPLOY_ENV=$DEPLOY_ENV)"
                TDIR="/root/app/attendance/${DEPLOY_ENV}"
                mkdir -p "$TDIR"
                # copy archive from Termux home (available at /data/home path forwarded) into /tmp inside distro
                if cp "$REMOTE_HOME/app/attendance/${DEPLOY_ENV}/deploy.tar.gz" "/tmp/deploy_${DEPLOY_ENV}.tar.gz" 2>/dev/null; then
                  tar -xzf "/tmp/deploy_${DEPLOY_ENV}.tar.gz" -C "$TDIR" || true
                  rm -f "/tmp/deploy_${DEPLOY_ENV}.tar.gz" || true
                else
                  echo "PROOT: could not copy archive into distro; maybe path not visible; attempting to extract directly from host path" || true
                fi

                cd "$TDIR" || exit 0

                # Ensure bun is available
                if command -v bun >/dev/null 2>&1; then
                  BUN="$(command -v bun)"
                else
                  echo "PROOT: bun not found in distro; please install bun inside the distro" >&2
                fi

                # Install dependencies and build if package.json exists
                if [ -f package.json ]; then
                  echo "PROOT: running bun install inside distro"
                  bun install || true
                  if grep -q '"build"' package.json 2>/dev/null; then
                    bun run build -- --preset bun || true
                  fi
                fi

                # Start the app via pm2 if available
                if command -v pm2 >/dev/null 2>&1; then
                  pm2 stop attendance-app || true; pm2 delete attendance-app || true
                  pm2 start --name attendance-app --interpreter "$BUN" ./.output/server/index.mjs --update-env || true
                  pm2 save || true
                else
                  echo "PROOT: pm2 not available; attempting to run bun directly in background"
                  nohup "$BUN" ./.output/server/index.mjs &>/dev/null &
                fi
              PROOT
            else
              echo "REMOTE: no archive found in $STAGE; skipping extract/build/start"
            fi
          SSH
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Notify
        run: echo "Deploy to ${TARGET_HOST} completed"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

      - name: Cleanup Tailscale and runner artifacts
        if: always()
        run: |
          echo "Cleaning up tailscale and temporary files"
          set -euo pipefail
          # Attempt to bring down tailscale and logout (idempotent)
          if command -v tailscale >/dev/null 2>&1; then
            sudo tailscale down || true
            sudo tailscale logout || true
          fi
          # Kill tailscaled if it's still running
          if pgrep -x tailscaled >/dev/null 2>&1; then
            sudo pkill -9 tailscaled || true
          fi
          # Remove tailscaled socket and temp logs we created
          sudo rm -f /var/run/tailscale/tailscaled.sock || true
          sudo rm -f /tmp/tailscaled.log /tmp/tailscale_up.err || true
          # Remove the private key from runner workspace
          rm -f ~/.ssh/id_rsa || true
          # Remove ssh config created earlier
          rm -f ~/.ssh/config || true
          echo "Cleanup finished"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

    # required secrets
    # secrets to set in repository settings:
    # - TAILSCALE_AUTHKEY
    # - SSH_PRIVATE_KEY (PEM formatted private key for TARGET_SSH_USER)
    # - TARGET_HOST (target device Tailscale IP or hostname reachable over tailnet)
    # - TARGET_SSH_USER (username to SSH as)
    # - TARGET_DEPLOY_PATH (optional remote path, defaults to /home/<user>/deploy)
