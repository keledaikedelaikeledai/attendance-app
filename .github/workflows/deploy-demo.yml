name: Deploy to demo via Tailscale

on:
  push:
    branches:
      - demo

permissions:
  contents: read

jobs:
  deploy:
    name: Build & deploy to Tailnet host
    runs-on: ubuntu-latest
    environment: demo
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          version: latest

      - name: Install dependencies
        run: bun install

      - name: Build (Nuxt + Nitro)
        run: bun run build -- --preset bun

      - name: Prepare deploy archive
        run: |
          # Create a stable snapshot of tracked files (git archive) into a temp dir,
          # then add build/runtime artifacts (.output, public) if present and compress.
          rm -rf deploy_tmp deploy.tar.gz
          mkdir -p deploy_tmp

          # Export tracked files at HEAD to the temp dir
          git archive HEAD | tar -x -C deploy_tmp

          # Copy build artifacts produced by the previous steps (if any)
          if [ -d .output ]; then
            cp -a .output deploy_tmp/.output
          fi
          if [ -d public ]; then
            cp -a public deploy_tmp/public
          fi

          # Create compressed archive from the stable temp dir
          tar -czf deploy.tar.gz -C deploy_tmp .
          rm -rf deploy_tmp

      - name: Install & start Tailscale
        run: |
          set -euo pipefail
          # Install tailscale binaries
          curl -fsSL https://tailscale.com/install.sh | sh

          # Start tailscaled only if not already running (avoid socket-in-use)
          if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
            echo "tailscaled already running; skipping start"
          else
            echo "Starting tailscaled..."
            sudo tailscaled --tun=userspace-networking &>/tmp/tailscaled.log &
            # wait for the socket or process to show up
            for i in 1 2 3 4 5; do
              if [ -S /var/run/tailscale/tailscaled.sock ] || pgrep -x tailscaled >/dev/null 2>&1; then
                break
              fi
              sleep 1
            done
          fi

          # Try to bring up tailscale using the provided auth key with retries.
          MAX_ATTEMPTS=3
          ATTEMPT=1
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS: tailscale up --authkey (non-interactive)"
            if sudo tailscale up --authkey "${TAILSCALE_AUTHKEY}" --hostname "github-runner-${{ github.run_id }}" 2>/tmp/tailscale_up.err; then
              echo "tailscale up succeeded"
              break
            else
              echo "tailscale up failed on attempt $ATTEMPT; dumping stderr (short):"
              tail -n +1 /tmp/tailscale_up.err || true
              if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
                echo "ERROR: The provided TAILSCALE_AUTHKEY appears invalid or expired. Create a reusable auth key in the Tailscale admin console and add it to GitHub Secrets as TAILSCALE_AUTHKEY." >&2
                # No point retrying invalid key
                break
              fi
              ATTEMPT=$((ATTEMPT+1))
              sleep $((ATTEMPT*2))
            fi
          done

          echo "--- tailscaled log (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscaled.log || true'
          echo "--- tailscale up stderr (tail) ---"
          sudo sh -c 'tail -n 200 /tmp/tailscale_up.err || true'

          echo "--- tailscale status ---"
          sudo tailscale status || true
          echo "--- tailscale ip -4 ---"
          sudo tailscale ip -4 || true

          # If we ended with an 'invalid key' message, exit non-zero
          if grep -qi "invalid key" /tmp/tailscale_up.err || grep -qi "not valid" /tmp/tailscale_up.err; then
            echo "Final error: TAILSCALE_AUTHKEY invalid or rejected. Aborting." >&2
            exit 1
          fi

        env:
          TAILSCALE_AUTHKEY: ${{ secrets.TAILSCALE_AUTHKEY }}

      - name: Configure SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          # disable strict host key checking for CI (optional; consider adding known_hosts)
          printf "Host *\n  StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null\n" > ~/.ssh/config

      - name: Ensure remote deploy directory exists
        run: |
          # Expand target deploy path on the runner so we don't rely on remote variable expansion
          REMOTE_DIR="${TARGET_DEPLOY_PATH:-/home/${TARGET_SSH_USER}/apps/attendance/demo}"
          echo "Ensuring remote dir: $REMOTE_DIR"
          # Prevent the ssh call from hanging indefinitely; timeout after 20s
          if ! timeout 20s ssh -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 ${TARGET_SSH_USER}@${TARGET_HOST} "mkdir -p '$REMOTE_DIR'" 2>/tmp/ssh_mkdir.err; then
            cat /tmp/ssh_mkdir.err || true
            if grep -q "Tailscale SSH requires an additional check" /tmp/ssh_mkdir.err 2>/dev/null || grep -q "Authentication check" /tmp/ssh_mkdir.err 2>/dev/null; then
              echo "ERROR: The target host is enforcing Tailscale SSH SSO and requires interactive authentication. CI cannot proceed non-interactively.\nOptions:\n - Use a host with standard OpenSSH and a deploy public key in ~/.ssh/authorized_keys\n - Configure Tailscale to allow machine authentication for CI or use a different deployment method." >&2
            else
              echo "ERROR: ssh to ${TARGET_HOST} failed or timed out. See /tmp/ssh_mkdir.err for details." >&2
            fi
            exit 1
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify Tailscale connectivity to target host
        run: |
          # Try a quick non-interactive SSH check to ensure the Tailscale route is available
          if ssh -o BatchMode=yes -o ConnectTimeout=10 -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} 'echo ok' 2>/dev/null; then
            echo "Target reachable over Tailscale"
          else
            echo "ERROR: Target ${TARGET_HOST} is not reachable over Tailscale from the runner. Ensure the TAILSCALE_AUTHKEY is valid and the host is online." >&2
            exit 1
          fi
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}

      - name: Copy archive to target host over Tailscale
        run: |
          REMOTE_PATH="${TARGET_DEPLOY_PATH:-/home/${TARGET_SSH_USER}/apps/attendance/demo}/deploy.tar.gz"
          echo "Copying archive to ${TARGET_SSH_USER}@${TARGET_HOST}:$REMOTE_PATH"
          scp -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=20 deploy.tar.gz ${TARGET_SSH_USER}@${TARGET_HOST}:"$REMOTE_PATH" 2>/tmp/scp.err || {
            cat /tmp/scp.err || true
            echo "ERROR: scp failed. Check that the deploy user has a matching public key and that Tailscale connectivity is available." >&2
            exit 1
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Verify uploaded archive exists on target
        run: |
          REMOTE_PATH="${TARGET_DEPLOY_PATH:-/home/${TARGET_SSH_USER}/apps/attendance/demo}/deploy.tar.gz"
          echo "Verifying archive exists at $REMOTE_PATH"
          ssh -o StrictHostKeyChecking=no -o BatchMode=yes -o ConnectTimeout=10 ${TARGET_SSH_USER}@${TARGET_HOST} "[ -f '$REMOTE_PATH' ] && echo OK || (echo 'ERROR: deploy.tar.gz not found on target' >&2; exit 1)" 2>/tmp/ssh_verify.err || {
            cat /tmp/ssh_verify.err || true
            if grep -q "Tailscale SSH requires an additional check" /tmp/ssh_verify.err 2>/dev/null; then
              echo "ERROR: Tailscale SSH is requesting interactive SSO authentication. CI cannot continue. See docs: use standard OpenSSH+authorized_keys or configure machine auth for CI." >&2
            fi
            exit 1
          }
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Extract & run docker-compose on target host
        run: |
          # Compute the remote directory on the runner so we inject a literal path into the remote script
          REMOTE_DIR="${TARGET_DEPLOY_PATH:-/home/${TARGET_SSH_USER}/apps/attendance/demo}"
          echo "Using remote directory: $REMOTE_DIR"
          echo "REMOTE_DIR length: ${#REMOTE_DIR}"
          echo "About to run: ssh -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} 'bash -s' -- '$REMOTE_DIR'"

          # Runner-side guard: fail early if REMOTE_DIR is empty (prevents calling ssh with an empty arg)
          if [ -z "$REMOTE_DIR" ]; then
            echo "ERROR: REMOTE_DIR resolved to an empty string on the runner. Aborting before SSH." >&2
            echo "Ensure TARGET_DEPLOY_PATH or TARGET_SSH_USER secrets are set and non-empty." >&2
            exit 2
          fi

          # Pass pre-expanded REMOTE_DIR as an argument to the remote bash to avoid expansion issues
          # Use a single-quoted heredoc so the local shell doesn't expand variables inside the script body
          ssh -o StrictHostKeyChecking=no ${TARGET_SSH_USER}@${TARGET_HOST} "bash -s" -- "$REMOTE_DIR" <<'SSH'
            set -euo pipefail
            TARGET_DIR="$1"
            # Diagnostic: echo the received TARGET_DIR and its byte-length
            printf 'REMOTE: Received TARGET_DIR=<%s>\n' "$TARGET_DIR"
            printf 'REMOTE: TARGET_DIR length: %d\n' "$(printf '%s' "$TARGET_DIR" | wc -c)"
            if [ -z "$TARGET_DIR" ]; then
              echo "ERROR: TARGET_DIR is empty on the remote side. Aborting." >&2
              exit 2
            fi
            mkdir -p "$TARGET_DIR"
            cd "$TARGET_DIR"

            # stop existing stack if present (try without sudo first, then with passwordless sudo)
            if docker compose ls >/dev/null 2>&1; then
              docker compose down || true
            elif sudo -n docker compose ls >/dev/null 2>&1; then
              sudo docker compose down || true
            else
              echo "ERROR: docker on the target host requires sudo password or is not available.\nPlease either add ${TARGET_SSH_USER} to the 'docker' group or enable passwordless sudo for docker commands."
              exit 1
            fi

            # extract uploaded archive into target dir (scp already copied it)
            tar -xzf "$TARGET_DIR/deploy.tar.gz" -C "$TARGET_DIR"
            rm -f "$TARGET_DIR/deploy.tar.gz"

            # run docker compose build+up (prefer direct docker, fall back to passwordless sudo)
            if docker compose up -d --build; then
              true
            elif sudo -n docker compose up -d --build; then
              true
            else
              echo "ERROR: docker on the target host requires sudo password or is not available. Aborting."
              exit 1
            fi
          SSH
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}
          TARGET_SSH_USER: ${{ secrets.TARGET_SSH_USER }}
          TARGET_DEPLOY_PATH: ${{ secrets.TARGET_DEPLOY_PATH }}

      - name: Notify
        run: echo "Deploy to ${TARGET_HOST} completed"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

      - name: Cleanup Tailscale and runner artifacts
        if: always()
        run: |
          echo "Cleaning up tailscale and temporary files"
          set -euo pipefail
          # Attempt to bring down tailscale and logout (idempotent)
          if command -v tailscale >/dev/null 2>&1; then
            sudo tailscale down || true
            sudo tailscale logout || true
          fi
          # Kill tailscaled if it's still running
          if pgrep -x tailscaled >/dev/null 2>&1; then
            sudo pkill -9 tailscaled || true
          fi
          # Remove tailscaled socket and temp logs we created
          sudo rm -f /var/run/tailscale/tailscaled.sock || true
          sudo rm -f /tmp/tailscaled.log /tmp/tailscale_up.err || true
          # Remove the private key from runner workspace
          rm -f ~/.ssh/id_rsa || true
          # Remove ssh config created earlier
          rm -f ~/.ssh/config || true
          echo "Cleanup finished"
        env:
          TARGET_HOST: ${{ secrets.TARGET_HOST }}

    # required secrets
    # secrets to set in repository settings:
    # - TAILSCALE_AUTHKEY
    # - SSH_PRIVATE_KEY (PEM formatted private key for TARGET_SSH_USER)
    # - TARGET_HOST (target device Tailscale IP or hostname reachable over tailnet)
    # - TARGET_SSH_USER (username to SSH as)
    # - TARGET_DEPLOY_PATH (optional remote path, defaults to /home/<user>/deploy)
